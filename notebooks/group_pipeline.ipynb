{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21029480",
   "metadata": {},
   "source": [
    "\n",
    "# Group Pipeline — Airline Delay Analysis (Preprocessing & EDA)\n",
    "\n",
    "\n",
    "**Pipeline order:**\n",
    "1. **IT24102978 — Handle Missing Data**\n",
    "2. **IT24102834 — Outlier Removal**\n",
    "3. **IT24102889 — Encode Categorical Variables**\n",
    "4. **IT24102942 — Feature Engineering**\n",
    "5. **IT24102856 — Normalization / Scaling**\n",
    "6. **IT24102979 — Dimensionality Reduction (PCA)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ac9e5e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1495252f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape BEFORE: (7422037, 21)\n",
      "Missing AFTER (top 10):\n",
      "FL_DATE              0\n",
      "OP_UNIQUE_CARRIER    0\n",
      "SECURITY_DELAY       0\n",
      "NAS_DELAY            0\n",
      "WEATHER_DELAY        0\n",
      "CARRIER_DELAY        0\n",
      "DISTANCE             0\n",
      "AIR_TIME             0\n",
      "ARR_DELAY            0\n",
      "ARR_TIME             0\n",
      "dtype: int64\n",
      "Shape AFTER missing-data: (7422037, 20)\n",
      "DEP_DELAY: removed 719539 outliers (k=3.0)\n",
      "ARR_DELAY: removed 46035 outliers (k=3.0)\n",
      "Shape AFTER outlier removal: (6656463, 20)\n",
      "Shape AFTER encoding: (6656463, 98)\n",
      "Shape AFTER feature engineering: (6656463, 102)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 5.06 GiB for an array with shape (6656463, 102) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 165\u001b[0m\n\u001b[0;32m    161\u001b[0m scale_cols \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m numeric_cols \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m exclude]\n\u001b[0;32m    163\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[0;32m    164\u001b[0m X_scaled \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m--> 165\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mfit_transform(df[scale_cols]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)),\n\u001b[0;32m    166\u001b[0m     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_z\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m scale_cols],\n\u001b[0;32m    167\u001b[0m     index\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mindex\n\u001b[0;32m    168\u001b[0m )\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScaled matrix shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, X_scaled\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# Save scaled features (optional)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    319\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1084\u001b[0m             (\n\u001b[0;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   1094\u001b[0m         )\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:878\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 878\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_fit(X, y, sample_weight)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:999\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    996\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_seen_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39misnan(X)\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    998\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 999\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvar_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_seen_ \u001b[38;5;241m=\u001b[39m _incremental_mean_and_var(\n\u001b[0;32m   1000\u001b[0m             X,\n\u001b[0;32m   1001\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_,\n\u001b[0;32m   1002\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvar_,\n\u001b[0;32m   1003\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_seen_,\n\u001b[0;32m   1004\u001b[0m             sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1005\u001b[0m         )\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;66;03m# for backward-compatibility, reduce n_samples_seen_ to an integer\u001b[39;00m\n\u001b[0;32m   1008\u001b[0m \u001b[38;5;66;03m# if the number of samples is the same for each feature (i.e. no\u001b[39;00m\n\u001b[0;32m   1009\u001b[0m \u001b[38;5;66;03m# missing values)\u001b[39;00m\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mptp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_seen_) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1143\u001b[0m, in \u001b[0;36m_incremental_mean_and_var\u001b[1;34m(X, last_mean, last_variance, last_sample_count, sample_weight)\u001b[0m\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1142\u001b[0m     T \u001b[38;5;241m=\u001b[39m new_sum \u001b[38;5;241m/\u001b[39m new_sample_count\n\u001b[1;32m-> 1143\u001b[0m     temp \u001b[38;5;241m=\u001b[39m X \u001b[38;5;241m-\u001b[39m T\n\u001b[0;32m   1144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1145\u001b[0m         \u001b[38;5;66;03m# equivalent to np.nansum((X-T)**2 * sample_weight, axis=0)\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m         \u001b[38;5;66;03m# safer because np.float64(X*W) != np.float64(X)*np.float64(W)\u001b[39;00m\n\u001b[0;32m   1147\u001b[0m         correction \u001b[38;5;241m=\u001b[39m _safe_accumulator_op(\n\u001b[0;32m   1148\u001b[0m             np\u001b[38;5;241m.\u001b[39mmatmul, sample_weight, np\u001b[38;5;241m.\u001b[39mwhere(X_nan_mask, \u001b[38;5;241m0\u001b[39m, temp)\n\u001b[0;32m   1149\u001b[0m         )\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 5.06 GiB for an array with shape (6656463, 102) and data type float64"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Setup / Imports ===\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Optional for later steps\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Paths (adjust if needed)\n",
    "RAW_PATH = \"C:\\\\Users\\\\Chandupa Weerakkody\\\\Documents\\\\AIML_LivePROJECT\\\\2019.csv\"\n",
    "OUT_DIR = \"C:\\\\Users\\\\Chandupa Weerakkody\\\\Documents\\\\AIML_LivePROJECT\\\\output\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# === IT24102978 — Handle Missing Data ===\n",
    "# Minimal, undergrad-friendly\n",
    "\n",
    "# 1) Load\n",
    "df = pd.read_csv(RAW_PATH)\n",
    "print(\"Shape BEFORE:\", df.shape)\n",
    "\n",
    "# 2) Drop fully empty columns (e.g., 'Unnamed: 20')\n",
    "df = df.dropna(axis=1, how=\"all\")\n",
    "\n",
    "# 3) Parse date\n",
    "if \"FL_DATE\" in df.columns:\n",
    "    df[\"FL_DATE\"] = pd.to_datetime(df[\"FL_DATE\"], errors=\"coerce\")\n",
    "\n",
    "# 4) Known delay-cause NaNs -> 0\n",
    "cause_cols = [\"CARRIER_DELAY\", \"WEATHER_DELAY\", \"NAS_DELAY\", \"SECURITY_DELAY\", \"LATE_AIRCRAFT_DELAY\"]\n",
    "present_cause = [c for c in cause_cols if c in df.columns]\n",
    "if present_cause:\n",
    "    df[present_cause] = df[present_cause].fillna(0)\n",
    "\n",
    "# 5) Impute remaining numeric/categorical\n",
    "num_cols = df.select_dtypes(include=\"number\").columns\n",
    "cat_cols = df.select_dtypes(include=\"object\").columns\n",
    "\n",
    "# numeric -> median\n",
    "df[num_cols] = df[num_cols].fillna(df[num_cols].median(numeric_only=True))\n",
    "\n",
    "# categorical -> mode\n",
    "for c in cat_cols:\n",
    "    if df[c].isna().any():\n",
    "        m = df[c].mode(dropna=True)\n",
    "        if not m.empty:\n",
    "            df[c] = df[c].fillna(m.iloc[0])\n",
    "\n",
    "print(\"Missing AFTER (top 10):\")\n",
    "print(df.isna().sum().sort_values(ascending=False).head(10))\n",
    "print(\"Shape AFTER missing-data:\", df.shape)\n",
    "\n",
    "# Save intermediate (optional)\n",
    "df.to_csv(os.path.join(OUT_DIR, \"after_missing.csv\"), index=False)\n",
    "\n",
    "\n",
    "# === IT24102834 — Outlier Removal ===\n",
    "# Remove extreme outliers from DEP_DELAY and ARR_DELAY using IQR*3 (conservative).\n",
    "# If columns are absent, this section will simply skip.\n",
    "\n",
    "def remove_outliers_iqr(data, col, k=3.0):\n",
    "    s = data[col].dropna()\n",
    "    if s.empty:\n",
    "        return data\n",
    "    q1, q3 = s.quantile(0.25), s.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    if iqr == 0:\n",
    "        return data\n",
    "    lower, upper = q1 - k*iqr, q3 + k*iqr\n",
    "    before = len(data)\n",
    "    data = data[(data[col].isna()) | ((data[col] >= lower) & (data[col] <= upper))].copy()\n",
    "    removed = before - len(data)\n",
    "    print(f\"{col}: removed {removed} outliers (k={k})\")\n",
    "    return data\n",
    "\n",
    "for col in [\"DEP_DELAY\", \"ARR_DELAY\"]:\n",
    "    if col in df.columns:\n",
    "        df = remove_outliers_iqr(df, col, k=3.0)\n",
    "\n",
    "print(\"Shape AFTER outlier removal:\", df.shape)\n",
    "\n",
    "# Save intermediate (optional)\n",
    "df.to_csv(os.path.join(OUT_DIR, \"after_outliers.csv\"), index=False)\n",
    "\n",
    "\n",
    "# === IT24102889 — Encode Categorical Variables ===\n",
    "# Simple: extract date parts; one-hot for CARRIER; top-30 for ORIGIN/DEST then one-hot.\n",
    "\n",
    "# 1) Date -> numeric\n",
    "if \"FL_DATE\" in df.columns:\n",
    "    df[\"YEAR\"] = df[\"FL_DATE\"].dt.year\n",
    "    df[\"MONTH\"] = df[\"FL_DATE\"].dt.month\n",
    "    df[\"DAY_OF_WEEK\"] = df[\"FL_DATE\"].dt.dayofweek  # Mon=0..Sun=6\n",
    "    # Drop raw date to keep numeric-only downstream\n",
    "    df = df.drop(columns=[\"FL_DATE\"])\n",
    "\n",
    "# 2) Limit airport cardinality\n",
    "for col in [\"ORIGIN\", \"DEST\"]:\n",
    "    if col in df.columns:\n",
    "        top = df[col].value_counts().head(30).index\n",
    "        df[col] = df[col].where(df[col].isin(top), \"OTHER\")\n",
    "\n",
    "# 3) One-hot encode selected categoricals\n",
    "cols_to_encode = [c for c in [\"OP_UNIQUE_CARRIER\", \"ORIGIN\", \"DEST\"] if c in df.columns]\n",
    "df = pd.get_dummies(df, columns=cols_to_encode,\n",
    "                    prefix=[\"CARRIER\", \"ORIGIN\", \"DEST\"],\n",
    "                    prefix_sep=\"_\", dtype=int)\n",
    "\n",
    "print(\"Shape AFTER encoding:\", df.shape)\n",
    "\n",
    "# Save intermediate (optional)\n",
    "df.to_csv(os.path.join(OUT_DIR, \"after_encoding.csv\"), index=False)\n",
    "\n",
    "\n",
    "# === IT24102942 — Feature Engineering ===\n",
    "# Create simple, interpretable features for later analysis/models.\n",
    "\n",
    "# Binary delay indicators\n",
    "if \"ARR_DELAY\" in df.columns:\n",
    "    df[\"IS_DELAYED_ARR\"] = (df[\"ARR_DELAY\"] > 0).astype(int)\n",
    "if \"DEP_DELAY\" in df.columns:\n",
    "    df[\"IS_DELAYED_DEP\"] = (df[\"DEP_DELAY\"] > 0).astype(int)\n",
    "\n",
    "# Weekend flag from DAY_OF_WEEK if present (Mon=0..Sun=6)\n",
    "if \"DAY_OF_WEEK\" in df.columns:\n",
    "    df[\"IS_WEEKEND\"] = df[\"DAY_OF_WEEK\"].isin([5, 6]).astype(int)\n",
    "\n",
    "# Sum of cause delays if present\n",
    "cause_cols = [\"CARRIER_DELAY\", \"WEATHER_DELAY\", \"NAS_DELAY\", \"SECURITY_DELAY\", \"LATE_AIRCRAFT_DELAY\"]\n",
    "present_cause = [c for c in cause_cols if c in df.columns]\n",
    "if present_cause:\n",
    "    df[\"TOTAL_DELAY_CAUSES\"] = df[present_cause].sum(axis=1)\n",
    "\n",
    "# Extract hour from CRS_DEP_TIME / CRS_ARR_TIME if they exist (times like HHMM)\n",
    "def to_hour(val):\n",
    "    try:\n",
    "        iv = int(val)\n",
    "        return iv // 100\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "for col, out_col in [(\"CRS_DEP_TIME\", \"CRS_DEP_HOUR\"), (\"CRS_ARR_TIME\", \"CRS_ARR_HOUR\")]:\n",
    "    if col in df.columns:\n",
    "        df[out_col] = df[col].apply(to_hour)\n",
    "\n",
    "print(\"Shape AFTER feature engineering:\", df.shape)\n",
    "\n",
    "# Save intermediate (optional)\n",
    "df.to_csv(os.path.join(OUT_DIR, \"after_features.csv\"), index=False)\n",
    "\n",
    "\n",
    "# === IT24102856 — Normalization / Scaling ===\n",
    "# Standardize numeric features into a separate DataFrame X_scaled (z-scores).\n",
    "# We keep the original df intact for interpretability.\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=\"number\").columns.tolist()\n",
    "\n",
    "# If you want to keep some columns unscaled (e.g., labels), exclude them here:\n",
    "exclude = []  # e.g., ['ARR_DELAY', 'DEP_DELAY']\n",
    "scale_cols = [c for c in numeric_cols if c not in exclude]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(df[scale_cols].fillna(0)),\n",
    "    columns=[f\"{c}_z\" for c in scale_cols],\n",
    "    index=df.index\n",
    ")\n",
    "\n",
    "print(\"Scaled matrix shape:\", X_scaled.shape)\n",
    "\n",
    "# Save scaled features (optional)\n",
    "X_scaled.to_csv(os.path.join(OUT_DIR, \"scaled_features.csv\"), index=False)\n",
    "\n",
    "\n",
    "\n",
    "# === IT24102979 — Dimensionality Reduction (PCA) ===\n",
    "# PCA on the standardized features (from previous step). We'll append PCA1/PCA2 to df.\n",
    "\n",
    "if 'X_scaled' in globals() and not X_scaled.empty:\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    comps = pca.fit_transform(X_scaled.values)\n",
    "    df['PCA1'] = comps[:, 0]\n",
    "    df['PCA2'] = comps[:, 1]\n",
    "    print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
    "else:\n",
    "    print(\"PCA skipped: no scaled features.\")\n",
    "\n",
    "print(\"Final dataframe shape:\", df.shape)\n",
    "\n",
    "# Save final processed dataset with PCA columns included\n",
    "final_out = os.path.join(OUT_DIR, \"final_processed_with_pca.csv\")\n",
    "df.to_csv(final_out, index=False)\n",
    "print(\"Saved final dataset ->\", final_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30562b40-9331-409b-bcd8-54573f105e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# Group Pipeline — Airline Delay Analysis (Preprocessing & EDA)\n",
    "# Memory-safe version (sampling for scaling + PCA)\n",
    "# Each section is labeled with the member's IT number.\n",
    "# ==============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Try to import sklearn; if unavailable, scaling/PCA are skipped.\n",
    "try:\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.decomposition import PCA\n",
    "    _HAS_SKLEARN = True\n",
    "except Exception:\n",
    "    _HAS_SKLEARN = False\n",
    "    print(\"Note: scikit-learn not found. Scaling and PCA will be skipped.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Paths (change if needed)\n",
    "# -----------------------------\n",
    "RAW_PATH = \"C:\\\\Users\\\\Chandupa Weerakkody\\\\Documents\\\\AIML_LivePROJECT\\\\2019.csv\"   # portable (recommended)\n",
    "# RAW_PATH = r\"C:\\Users\\Chandupa Weerakkody\\Documents\\AIML_LivePROJECT\\data\\raw\\2019.csv\"  # absolute option\n",
    "\n",
    "OUT_DIR = \"C:\\\\Users\\\\Chandupa Weerakkody\\\\Documents\\\\AIML_LivePROJECT\\\\output\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Limit rows for scaling/PCA to avoid MemoryError (set None to disable sampling)\n",
    "MAX_ROWS_FOR_SCALING = 300_000\n",
    "ONEHOT_PREFIXES = (\"CARRIER_\", \"ORIGIN_\", \"DEST_\")  # columns to exclude from scaling\n",
    "\n",
    "# ==============================================================\n",
    "# IT24102978 — Handle Missing Data\n",
    "# ==============================================================\n",
    "print(\"\\n=== IT24102978 — Handle Missing Data ===\")\n",
    "df = pd.read_csv(RAW_PATH)\n",
    "print(\"Shape BEFORE:\", df.shape)\n",
    "\n",
    "# Drop fully empty columns (e.g., 'Unnamed: 20')\n",
    "df = df.dropna(axis=1, how=\"all\")\n",
    "\n",
    "# Parse date if present\n",
    "if \"FL_DATE\" in df.columns:\n",
    "    df[\"FL_DATE\"] = pd.to_datetime(df[\"FL_DATE\"], errors=\"coerce\")\n",
    "\n",
    "# Known delay-cause NaNs -> 0 (NaN ~= no delay attributed to that cause)\n",
    "cause_cols = [\"CARRIER_DELAY\",\"WEATHER_DELAY\",\"NAS_DELAY\",\"SECURITY_DELAY\",\"LATE_AIRCRAFT_DELAY\"]\n",
    "present_cause = [c for c in cause_cols if c in df.columns]\n",
    "if present_cause:\n",
    "    df[present_cause] = df[present_cause].fillna(0)\n",
    "\n",
    "# Impute remaining numeric/categorical\n",
    "num_cols = df.select_dtypes(include=\"number\").columns\n",
    "cat_cols = df.select_dtypes(include=\"object\").columns\n",
    "\n",
    "# numeric -> median\n",
    "df[num_cols] = df[num_cols].fillna(df[num_cols].median(numeric_only=True))\n",
    "\n",
    "# categorical -> mode\n",
    "for c in cat_cols:\n",
    "    if df[c].isna().any():\n",
    "        m = df[c].mode(dropna=True)\n",
    "        if not m.empty:\n",
    "            df[c] = df[c].fillna(m.iloc[0])\n",
    "\n",
    "print(\"Missing AFTER (top 10):\")\n",
    "print(df.isna().sum().sort_values(ascending=False).head(10))\n",
    "print(\"Shape AFTER missing-data:\", df.shape)\n",
    "\n",
    "df.to_csv(os.path.join(OUT_DIR, \"after_missing.csv\"), index=False)\n",
    "print(\"Saved ->\", os.path.join(OUT_DIR, \"after_missing.csv\"))\n",
    "\n",
    "# ==============================================================\n",
    "# IT24102834 — Outlier Removal\n",
    "# ==============================================================\n",
    "print(\"\\n=== IT24102834 — Outlier Removal (IQR * 3 on DEP_DELAY/ARR_DELAY) ===\")\n",
    "\n",
    "def remove_outliers_iqr(data, col, k=3.0):\n",
    "    s = data[col].dropna()\n",
    "    if s.empty:\n",
    "        return data\n",
    "    q1, q3 = s.quantile(0.25), s.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    if iqr == 0:\n",
    "        return data\n",
    "    lower, upper = q1 - k*iqr, q3 + k*iqr\n",
    "    before = len(data)\n",
    "    data = data[(data[col].isna()) | ((data[col] >= lower) & (data[col] <= upper))].copy()\n",
    "    print(f\"{col}: removed {before - len(data)} outliers (k={k})\")\n",
    "    return data\n",
    "\n",
    "for col in [\"DEP_DELAY\", \"ARR_DELAY\"]:\n",
    "    if col in df.columns:\n",
    "        df = remove_outliers_iqr(df, col, k=3.0)\n",
    "\n",
    "print(\"Shape AFTER outlier removal:\", df.shape)\n",
    "df.to_csv(os.path.join(OUT_DIR, \"after_outliers.csv\"), index=False)\n",
    "print(\"Saved ->\", os.path.join(OUT_DIR, \"after_outliers.csv\"))\n",
    "\n",
    "# ==============================================================\n",
    "# IT24102889 — Encode Categorical Variables\n",
    "# ==============================================================\n",
    "print(\"\\n=== IT24102889 — Encode Categorical Variables ===\")\n",
    "\n",
    "# Date parts from FL_DATE if present\n",
    "if \"FL_DATE\" in df.columns:\n",
    "    df[\"YEAR\"] = df[\"FL_DATE\"].dt.year\n",
    "    df[\"MONTH\"] = df[\"FL_DATE\"].dt.month\n",
    "    df[\"DAY_OF_WEEK\"] = df[\"FL_DATE\"].dt.dayofweek  # Mon=0..Sun=6\n",
    "    df = df.drop(columns=[\"FL_DATE\"])\n",
    "\n",
    "# Limit airport cardinality (keep top-30; others -> 'OTHER')\n",
    "TOP_K_AIRPORTS = 30\n",
    "for col in [\"ORIGIN\", \"DEST\"]:\n",
    "    if col in df.columns:\n",
    "        top = df[col].value_counts().head(TOP_K_AIRPORTS).index\n",
    "        df[col] = df[col].where(df[col].isin(top), \"OTHER\")\n",
    "\n",
    "# One-hot encode selected categoricals (simple)\n",
    "cols_to_encode = [c for c in [\"OP_UNIQUE_CARRIER\",\"ORIGIN\",\"DEST\"] if c in df.columns]\n",
    "if cols_to_encode:\n",
    "    df = pd.get_dummies(df, columns=cols_to_encode, dtype=int)\n",
    "\n",
    "print(\"Shape AFTER encoding:\", df.shape)\n",
    "df.to_csv(os.path.join(OUT_DIR, \"after_encoding.csv\"), index=False)\n",
    "print(\"Saved ->\", os.path.join(OUT_DIR, \"after_encoding.csv\"))\n",
    "\n",
    "# ==============================================================\n",
    "# IT24102942 — Feature Engineering\n",
    "# ==============================================================\n",
    "print(\"\\n=== IT24102942 — Feature Engineering ===\")\n",
    "\n",
    "# Binary delay indicators\n",
    "if \"ARR_DELAY\" in df.columns:\n",
    "    df[\"IS_DELAYED_ARR\"] = (df[\"ARR_DELAY\"] > 0).astype(int)\n",
    "if \"DEP_DELAY\" in df.columns:\n",
    "    df[\"IS_DELAYED_DEP\"] = (df[\"DEP_DELAY\"] > 0).astype(int)\n",
    "\n",
    "# Weekend flag\n",
    "if \"DAY_OF_WEEK\" in df.columns:\n",
    "    df[\"IS_WEEKEND\"] = df[\"DAY_OF_WEEK\"].isin([5, 6]).astype(int)\n",
    "\n",
    "# Sum of cause delays if present\n",
    "present_cause = [c for c in cause_cols if c in df.columns]\n",
    "if present_cause:\n",
    "    df[\"TOTAL_DELAY_CAUSES\"] = df[present_cause].sum(axis=1)\n",
    "\n",
    "# Extract hour from CRS times (HHMM -> hour)\n",
    "def to_hour(val):\n",
    "    try:\n",
    "        iv = int(val)\n",
    "        return iv // 100\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "for src, out in [(\"CRS_DEP_TIME\",\"CRS_DEP_HOUR\"), (\"CRS_ARR_TIME\",\"CRS_ARR_HOUR\")]:\n",
    "    if src in df.columns:\n",
    "        df[out] = df[src].apply(to_hour)\n",
    "\n",
    "print(\"Shape AFTER feature engineering:\", df.shape)\n",
    "df.to_csv(os.path.join(OUT_DIR, \"after_features.csv\"), index=False)\n",
    "print(\"Saved ->\", os.path.join(OUT_DIR, \"after_features.csv\"))\n",
    "\n",
    "# ==============================================================\n",
    "# IT24102856 — Normalization / Scaling (MEMORY-SAFE)\n",
    "# ==============================================================\n",
    "print(\"\\n=== IT24102856 — Normalization / Scaling (memory-safe) ===\")\n",
    "X_scaled = None\n",
    "if _HAS_SKLEARN:\n",
    "    numeric_cols = df.select_dtypes(include=\"number\").columns.tolist()\n",
    "\n",
    "    # Exclude one-hot columns from scaling (they start with these prefixes)\n",
    "    scale_cols = [c for c in numeric_cols if not any(c.startswith(p) for p in ONEHOT_PREFIXES)]\n",
    "\n",
    "    # Choose working rows (sample to fit in memory)\n",
    "    if (MAX_ROWS_FOR_SCALING is not None) and (len(df) > MAX_ROWS_FOR_SCALING):\n",
    "        work_idx = df.sample(MAX_ROWS_FOR_SCALING, random_state=42).index\n",
    "        print(f\"Sampling {len(work_idx):,} rows for scaling from {len(df):,}.\")\n",
    "    else:\n",
    "        work_idx = df.index\n",
    "\n",
    "    work_X = df.loc[work_idx, scale_cols].fillna(0)\n",
    "\n",
    "    # Downcast to float32 to reduce memory (sklearn upcasts internally, but this still helps I/O)\n",
    "    try:\n",
    "        work_X = work_X.astype(\"float32\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(work_X.values)\n",
    "    # Save sample-scaled features (not the full 6.6M rows)\n",
    "    scaled_sample = pd.DataFrame(X_scaled, columns=[f\"{c}_z\" for c in scale_cols], index=work_idx)\n",
    "    scaled_sample.to_csv(os.path.join(OUT_DIR, \"scaled_features_SAMPLE.csv\"))\n",
    "    print(\"Scaled SAMPLE shape:\", scaled_sample.shape)\n",
    "    print(\"Saved ->\", os.path.join(OUT_DIR, \"scaled_features_SAMPLE.csv\"))\n",
    "else:\n",
    "    print(\"Skipped: scikit-learn not installed.\")\n",
    "\n",
    "# ==============================================================\n",
    "# IT24102979 — Dimensionality Reduction (PCA) — on scaled SAMPLE\n",
    "# ==============================================================\n",
    "print(\"\\n=== IT24102979 — Dimensionality Reduction (PCA on sample) ===\")\n",
    "if _HAS_SKLEARN and (X_scaled is not None):\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    comps = pca.fit_transform(X_scaled)  # only on the scaled sample\n",
    "    pca_df = pd.DataFrame({\"PCA1\": comps[:, 0], \"PCA2\": comps[:, 1]}, index=work_idx)\n",
    "\n",
    "    # Attach PCA1/2 back to the main df (others remain NaN)\n",
    "    df.loc[work_idx, \"PCA1\"] = pca_df[\"PCA1\"]\n",
    "    df.loc[work_idx, \"PCA2\"] = pca_df[\"PCA2\"]\n",
    "\n",
    "    # Also save PCA components (sample only)\n",
    "    pca_df.to_csv(os.path.join(OUT_DIR, \"pca_components_SAMPLE.csv\"))\n",
    "    print(\"PCA sample shape:\", pca_df.shape)\n",
    "    print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
    "    print(\"Saved ->\", os.path.join(OUT_DIR, \"pca_components_SAMPLE.csv\"))\n",
    "else:\n",
    "    print(\"Skipped PCA: no scaled sample available or scikit-learn missing.\")\n",
    "\n",
    "print(\"\\nFinal dataframe shape:\", df.shape)\n",
    "final_out = os.path.join(OUT_DIR, \"final_processed_with_pca.csv\")\n",
    "df.to_csv(final_out, index=False)\n",
    "print(\"Saved final dataset ->\", final_out)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
